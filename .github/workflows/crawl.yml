name: Scheduled Web Crawling

on:
  # Run on schedule
  schedule:
    - cron: '0 0 * * *'  # Run daily at midnight UTC
  
  # Allow manual triggering
  workflow_dispatch:
    inputs:
      urls:
        description: 'URLs to crawl (comma separated)'
        required: false
        default: ''
      depth:
        description: 'Crawl depth'
        required: false
        default: '3'
      crawlers:
        description: 'Crawlers to use (comma separated: desktop,mobile,image)'
        required: false
        default: 'desktop,mobile,image'

jobs:
  crawl:
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v3
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
          cache: 'pip'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
      
      - name: Install Chrome
        run: |
          sudo apt-get update
          sudo apt-get install -y google-chrome-stable
      
      - name: Create logs directory
        run: |
          mkdir -p logs
      
      - name: Run Crawler with provided URLs
        if: ${{ github.event.inputs.urls != '' }}
        run: |
          # Parse input parameters
          IFS=',' read -ra URL_ARRAY <<< "${{ github.event.inputs.urls }}"
          
          # Modify config based on input
          echo "Modifying configuration based on inputs"
          PYTHONPATH=$PYTHONPATH:$(pwd) python scripts/github_actions/modify_config.py "${{ github.event.inputs.crawlers }}" "${{ github.event.inputs.depth }}"
          
          # Run the crawler for each URL
          for url in "${URL_ARRAY[@]}"; do
            echo "Crawling $url"
            PYTHONPATH=$PYTHONPATH:$(pwd) python scripts/github_actions/run_crawler.py "$url" "config_modified.yml"
          done
        env:
          CLIENT_SECRET: ${{ secrets.CLIENT_SECRET }}
      
      - name: Run Crawler with default URLs
        if: ${{ github.event.inputs.urls == '' }}
        run: |
          PYTHONPATH=$PYTHONPATH:$(pwd) python scripts/github_actions/run_crawler.py
        env:
          CLIENT_SECRET: ${{ secrets.CLIENT_SECRET }}
      
      - name: Build GitHub Pages
        run: |
          # Create docs directory if it doesn't exist
          mkdir -p docs
          
          # Move data to docs directory for GitHub Pages
          cp -r data/* docs/
          
          # Create index.html if it doesn't exist
          if [ ! -f docs/index.html ]; then
            cp scripts/github_actions/index_template.html docs/index.html
          fi
          
          # Create a JSON index file of all results
          PYTHONPATH=$PYTHONPATH:$(pwd) python scripts/github_actions/build_index.py
        env:
          CLIENT_SECRET: ${{ secrets.CLIENT_SECRET }}
      
      - name: Deploy to GitHub Pages
        uses: peaceiris/actions-gh-pages@v3
        with:
          github_token: ${{ secrets.GITHUB_TOKEN }}
          publish_dir: ./docs
          force_orphan: true 